**状态：** 已存档 (代码所见即所得，已经不再更新)

包含以上这些环境以及一些修正的内容已经被囊括进PettingZoo这个仓库里了。

PettingZoo (https://github.com/Farama-Foundation/PettingZoo , https://www.pettingzoo.ml/mpe)

# 多智能体离散粒子环境

一个简单的多智能体粒子仿真环境，该环境中的智能体观测到的状态为连续的，动作则为离散的，除此之外，该环境还包含了若干物理机制。
在此文章中被应用： [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf).

## 代码的结构

- `make_env.py`: 将本项目提供的多智能体离散粒子环境的代码打包为一个OpenAI Gym类的对象以方便用户使用。

- `./multiagent/environment.py`:用于环境模拟的代码(多智能体粒子之间的交互机制，`_step()` 步进函数等等)

- `./multiagent/core.py`: 包括了很多类在整个项目中被常常使用的对象 (实体，地标，多智能体等等) 。

- `./multiagent/rendering.py`: 用于在屏幕上显示多智能体的运动状态。

- `./multiagent/policy.py`: 这些代码实现了通过键盘输入控制的可交互策略。

- `./multiagent/scenario.py`: 包含了最基础的使用场景，该使用场景是其他所有使用场景的基场景，其他场景由基场景衍生。

- `./multiagent/scenarios/`:存储了不同场景与环境的文件夹， 场景代码包括以下一些函数：
  
  1) `make_world()`:生成环境中存在的所有对象 (地标、智能体等等)，分配他们的能力(比如说他们是否能够通信，是否能够移动等等)。能力与特性可以对标方法与属性，前者是一个智能体可以使用的函数（成员方法），后者是一个智能体本身具有的属性（成员变量）。
     make_world函数在每一个训练进程中只调用一次。
  
  2) `reset_world()`: 对环境中存在的所有实体赋予特性（例如实体的初始位置，颜色等等），赋予特性后完成环境重置。
     
     在每一次迭代前都会进行一次调用，而在完成环境生成后的第一次迭代前也会调用
     
     一次环境生成方法。
  
  3) `reward()`: 定义了给定的一个智能体的奖励函数。
  
  4) `observation()`: 定义了给定的一个智能体的观察函数。
  
  5) (可选) `benchmark_data()`: 为在当前环境中训练出来的策略提供一个验证数据（例如验证指标）。

### 创建新的仿真场景

通过对以上提到过的四个程序进行改写，我们可以创建新的仿真场景 (`make_world()`, `reset_world()`, `reward()`, and `observation()`).

## 已有的仿真场景

| 在代码（对应论文里）中的场景名称                                         | 通信？ | 竞争？ | 备注                                                                                                                                                                                                                                        |
| -------------------------------------------------------- | --- |:--- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `simple.py`                                              | N   | N   | 1个智能体观察地标位置，智能体离地标越近奖励就越大。此场景不是一个多智能体的场景，一般用于为策略的设计debug。                                                                                                                                                                                 |
| `simple_adversary.py` (Physical deception)               | N   | Y   | 1 个敌方智能体（红色），N 个友方智能体（绿色） N 个地标 (一般设 N=2)。每一个智能体都会观察每一个地标与其他智能体的位置。一个地标被设为“目标地标”并被标为绿色。 友方智能体离目标地标越近则获得越高的奖励，与此同时，敌方智能体离目标越近时，友方智能体将会遭受惩罚。 敌方智能体离目标地标越近，则敌方智能体将获得更高的奖励，但敌方智能体并不知道若干地标中哪一个才是目标地标。因此，所有的友方智能体应当学会分散开来并尝试掩盖所有的地标来扰乱敌方智能体的感知。 |
| `simple_crypto.py` (Covert communication)                | Y   | Y   | 2个友方智能体（经典的通信或密码学问题设定中的爱丽丝与鲍勃）， 1个敌方智能体（伊芙）爱丽丝必须通过一个公开信道传递保密消息给鲍勃， 如果鲍勃将保密消息重建得越好，则爱丽丝和鲍勃将会得到更高的奖励， 与此同时，若伊芙将保密消息重建得越好，则爱丽丝和鲍勃会受到惩罚。爱丽丝和鲍勃拥有一对私钥，该私钥将在每次迭代前随机生成，爱丽丝与鲍勃必须学会利用私钥去对私密消息进行加密。                                                 |
| `simple_push.py` (Keep-away)                             | N   | Y   | 1个友方智能体，1个敌方智能体，1个地标。友方智能体离地标更近的时候将会获得更高的奖励。 敌方智能体离地标更近的时候将会获得更高的奖励，与此同时，当友方智能体离地标越远的时候，敌方智能体也能获得更高的奖励。所以敌方智能体将会学习如何将友方智能体推离地标。                                                                                                           |
| `simple_reference.py`                                    | Y   | N   | 2个智能体，3个不同颜色的地标。每个智能体的目的是接近它们各自的目的地标，但是每个智能体各自的目的地标是哪个却只被另一个智能体知晓。奖励是团体共有的。所以智能体们必须针对各自的目标地标信息进行互相交流并导航到自己的目标地标去。这个场景与 simple_speaker_listener 场景类似，但在本场景中，智能体既是信息发送者，又是信息接收者。                                                            |
| `simple_speaker_listener.py` (Cooperative communication) | Y   | N   | 与 simple_reference相似, 不同点是2个智能体中的1个停在原地不会移动并充当信息发送者，它被标为灰色，与此同时，2个智能体当中的另1个充当信息接收者，它不能发送信息，但是要通过接收信息来导航至自己的目的地标。                                                                                                                          |
| `simple_spread.py` (Cooperative navigation)              | N   | N   | N个智能体，N个地标。 智能体的奖励取决于所有智能体-地标组成的距离的组合当中最远的距离，该最大距离越小，则奖励越高。如果智能体与其他的智能体发生碰撞，则所有的智能体都会受到惩罚。所以，智能体们需要学会在覆盖所有地标的同时避免碰撞。                                                                                                                      |
| `simple_tag.py` (Predator-prey)                          | N   | Y   | 捕猎者-猎物场景。被标为绿色的友方智能体的行动速度更快，并且需要避免被标为红色的地方智能体接触到。敌方智能体运动得更慢并且想要接触到友方智能体。形状较大的黑色圆圈被设为障碍物，障碍物会堵住智能体的行动路径。                                                                                                                                   |
| `simple_world_comm.py`                                   | Y   | Y   | 本场景是伴随着论文介绍出现在视频里的场景。与simple_tag类似，但若干点不同（1）本场景中存在被标为小型蓝色球的食物存在，当友方离食物越近就会得到更高的奖励。（2）在本场景中存在“森林”地形，当智能体进入森林时，森林外的智能体将不能发现森林中的智能体。（3）存在一个”敌方智能体领导者“，该领导者可以在整个游戏运行时观察到环境中的所有智能体，与此同时，该”敌方智能体领导者“可以与其他敌方智能体进行通信以完成对追逐任务进行协调。                 |
